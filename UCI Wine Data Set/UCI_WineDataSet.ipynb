{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI Machine Learning Repository: Wine Data Set (Octave)\n",
    "\n",
    "My first project after completing Andrew Ng's ML course on coursera. Consequently, my approach is guided by the ideas and techniques covered there. The data set is from UCI's Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/wine).\n",
    "\n",
    "NOTE: The code is in Octave!\n",
    "\n",
    "## What's it about?\n",
    "\n",
    "From the UCI page:\n",
    "\"These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\"\n",
    "\n",
    "We have three classes, thirteen features, and 178 examples. Those are the features:\n",
    "\n",
    " 1) Alcohol\n",
    " 2) Malic acid\n",
    " 3) Ash\n",
    " 4) Alcalinity of ash\n",
    " 5) Magnesium\n",
    " 6) Total phenols\n",
    " 7) Flavanoids\n",
    " 8) Nonflavanoid phenols\n",
    " 9) Proanthocyanins\n",
    " 10) Color intensity\n",
    " 11) Hue\n",
    " 12) OD280/OD315 of diluted wines\n",
    " 13) Proline\n",
    " \n",
    "And the class distribution:\n",
    "\n",
    "Class 1: 59 examples, Class 2: 71, Class 3: 48.\n",
    " \n",
    "Our goal is to predict from which cultivar (class) a given example is from, i.e. we have a classification problem.\n",
    " \n",
    " Let's get started!\n",
    " \n",
    " ## 1. Loading and initializing the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear; close all; clc;\n",
    "\n",
    "format short g;  # this shows numbers as decimals instead of exponential numbers (i.e. 0.000.... instead of 1e-6)\n",
    "load wine.data.txt\n",
    "data = wine_data;\n",
    "l = length(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we print out the classes (first column of the data) to see if they're ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(:, 1)\n",
    "\n",
    "ans =\n",
    "\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           1\n",
    "           ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data are ordered by class, so we need to reshuffle them. An added benefit is that it reshuffles the data every time you re-run the algorithm, so over time you get a more robust idea of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndIDX = randperm(l); \n",
    "data = data(rndIDX, :);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define X, y and the number of features for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data(:, 2:end);\n",
    "y = data(:, 1);\n",
    "num_features = size(X, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspecting the data\n",
    "\n",
    "Before working the data, we want to get an impression of it. Let's take a look at the first rows as well as at the means and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the dataset\n",
    "fprintf(\"Data size: \\n\")\n",
    "size(data)\n",
    "fprintf(\"\\nFirst 10 rows: \\n\")\n",
    "data(1:10, :)\n",
    "fprintf(\"\\nMeans: \\n\")\n",
    "mean(X)\n",
    "fprintf(\"\\nStandard deviations: \\n\")\n",
    "std(X)\n",
    "fprintf(\"\\nPress Enter to continue. \\n\")\n",
    "pause;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(add data)\n",
    "\n",
    "We see that the data is now randomly ordered. Also, the features have different scales, so depending on the algorithm we want to use we have to scale them.\n",
    "\n",
    "The next step is to create some scatterplots to see how the examples are distributed for the different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range = linspace(1, l, l);\n",
    "scatter(range, X(:,1), 50, y) #X(:,1) is the first feature, X(:,2) the second etc. You can also use a loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the plots look like:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Alcohol.jpg\" width=\"500\" height=\"500\" align=\"left\"/>\n",
    "<img src=\"Malic%20Acid.jpg\" width=\"500\" height=\"500\" align=\"right\"/>\n",
    "<img src=\"Ash.jpg\" width=\"500\" height=\"500\" align=\"left\"/>\n",
    "<img src=\"Alcalinity%20of%20Ash.jpg\" width=\"500\" height=\"500\" align=\"right\"/>\n",
    "<img src=\"Magnesium.jpg\" width=\"500\" height=\"500\" align=\"left\"/>\n",
    "<img src=\"Phenols.jpg\" width=\"500\" height=\"500\" align=\"right\"/>\n",
    "<img src=\"Flavanoids.jpg\" width=\"500\" height=\"500\" align=\"left\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for many features (e.g. Flavanoids and Phenols), there are \"clusters\" formed by the different locations - there is systematic variance between the different classes. This is good news: With those differences between classes, a learner should not have any difficulties in becoming very accurate.\n",
    "\n",
    "With this in mind, we can proceed to train our algorithm! We've already reshuffled the data set. Now we bring all features to the same scale and split the data into training, validation, and test set.\n",
    "\n",
    "## 3. Setting up training, validation, and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features have different scales, so we want to normalize them\n",
    "# featureNormalize calculates means & standard deviations and scales each feature so that mean = 0, std = 1.\n",
    "# The code is in the folder\n",
    "[X_norm, mu, sigma] = featureNormalize(X);\n",
    "\n",
    "# we also replace X with X_norm\n",
    "X = X_norm;\n",
    "% Add a column of ones to x (intercept term)\n",
    "X = [ones(l, 1), X];\n",
    "\n",
    "# defining the training set (~60% of sample)\n",
    "XTrain = X(1:108, :);\n",
    "yTrain = y(1:108, :);\n",
    "m = length(XTrain);\n",
    "# cross-validation set (~20% of sample)\n",
    "XVal = X(109:143, :);\n",
    "yVal = y(109:143, :);\n",
    "n = length(XVal);\n",
    "# test set (~20% of sample)\n",
    "XTest = X(144:end, :);\n",
    "yTest = y(144:end, :);\n",
    "o = length(XTest);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up the parameters - choosing the learner\n",
    "\n",
    "In his coursera class, Andrew Ng advises to always start with a \"quick and dirty\" algorithm to get an idea about the things that will determine what to work on further: How accurate is the learner? Is there high bias or high variance? Is the cost descending with more iterations?\n",
    "\n",
    "Therefore, we'll start with a simple logistic regression. We have to predict each class separately, with y=1 when the class is true and y=0 when the other classes are true. So we actually train three learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will train three classifiers. Therefore, we need three thetas\n",
    "theta_one = zeros(size(X,2), 1);\n",
    "theta_two = zeros(size(X,2), 1); \n",
    "theta_three = zeros(size(X,2), 1);\n",
    "\n",
    "# you could also put the next few lines in the cost function, but I prefer to leave it in the main script:\n",
    "\n",
    "# this step will return a binary vector for each winery, with 1 if the class/winery is true and 0 if not\n",
    "# it allows us to train a logReg algorithm for each SK separately\n",
    "y_one = yTrain == 1;\n",
    "y_two = yTrain == 2;\n",
    "y_three = yTrain == 3;\n",
    "\n",
    "# same for yval\n",
    "yVal_one = yVal == 1;\n",
    "yVal_two= yVal == 2;\n",
    "yVal_three = yVal == 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we set lambda and call the cost functions. As a start I set lambda to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting lambda\n",
    "# if necessary, we can set different lambdas for the different classes\n",
    "lambda = 1;\n",
    "\n",
    "\n",
    "# defining the cost functions for each class\n",
    "[J_one, grad_one] = lrCostFunction(theta_one, XTrain, y_one, lambda);\n",
    "[J_two, grad_two] = lrCostFunction(theta_two, XTrain, y_two, lambda);\n",
    "[J_three, grad_three] = lrCostFunction(theta_three, XTrain, y_three, lambda);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the algorithm\n",
    "\n",
    "Alright, time to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the fminunc algorithm\n",
    "% Set options for fminunc\n",
    "options = optimset('GradObj', 'on', 'MaxIter', 1000);\n",
    "initial_theta = zeros(size(X,2), 1);\n",
    "\n",
    "% Run fminunc to obtain the optimal theta\n",
    "% This function will return theta and the cost for each theta\n",
    "[theta_one, cost_one] = ...\n",
    "fminunc(@(t)(lrCostFunction(t, XTrain, y_one, lambda)), initial_theta, options);\n",
    "[theta_two, cost_two] = ...\n",
    "fminunc(@(t)(lrCostFunction(t, XTrain, y_two, lambda)), initial_theta, options);\n",
    "[theta_three, cost_three] = ...\n",
    "fminunc(@(t)(lrCostFunction(t, XTrain, y_three, lambda)), initial_theta, options);\n",
    "\n",
    "\n",
    "# we also want to keep the parameters to inspect them\n",
    "theta = [theta_one theta_two theta_three];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a first impression of the results we print the thetas and the costs of the three learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the thetas gives us an idea of how the different features are related to the\n",
    "# different classes\n",
    "fprintf(\"Parameters: \\n\")\n",
    "theta\n",
    "\n",
    "fprintf(\"Press Enter to continue. \\n\")\n",
    "pause;\n",
    "\n",
    "# lets also see the costs at the end of the training\n",
    "fprintf(\"Costs for the training set: \\n\")\n",
    "cost_one\n",
    "cost_two\n",
    "cost_three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta =\n",
    "\n",
    "     -1.7237     -1.4759     -3.0199\n",
    "      1.2028     -1.4288     0.42278\n",
    "     0.11032     -0.4326     0.50957\n",
    "      1.0943     -1.0148     0.24107\n",
    "     -1.2839     0.66659     0.23025\n",
    "   -0.058025    0.046484   -0.022954\n",
    "     0.32387     0.26197    -0.53554\n",
    "     0.99897     0.32284     -1.1591\n",
    "    -0.24578     0.08592     0.34017\n",
    "  -0.0018193    0.033399    -0.63712\n",
    "    0.072398     -1.7596      1.1497\n",
    "       0.199     0.90641      -1.009\n",
    "     0.86185   -0.071956    -0.91172\n",
    "      1.4149     -1.5651   -0.035918\n",
    "\n",
    "Press Enter to continue.\n",
    "Costs for the training set:\n",
    "cost_one =   0.075485\n",
    "cost_two =    0.11022\n",
    "cost_three =   0.048113"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can seet that the costs are already pretty low. That's a good sign unless we're seriously overfitting our training set. Well, we're about to find out!\n",
    "\n",
    "## 6. Evaluating the algorithm\n",
    "\n",
    "A nice way to get a better idea about the learner's performance is to plot a learning curve. For this curve, we start training a learner with just one example and measure the error for the training and the validation set. Then we take two examples and do the same, up until we train it with the whole test set. The curve will show us if the errors drop as the training set gets bigger. It'll also show the difference between training and test set errors and their development - this allows us to judge the bias and variance of the algorithm.\n",
    "\n",
    "For lambda = 1, those are the learning curves:\n",
    "\n",
    "<img src=\"LearningCurveLambda1.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<img src=\"LearningCurveLambda2.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<img src=\"LearningCurveLambda3.jpg\" width=\"500\" height=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us? The cost for both sets is fairly low. However, the differences between training and validation sets indicate overfitting. Furthermore, the error stops to decrease after 40-60 examples. So we could also have high bias and underfit our training data. Let's continue to investigate this.\n",
    "\n",
    "One parameter we can adjust very easily is lambda. We can compare performance on the training and validation sets for different values. This will allow us to see if we're really overfitting the training data: In this case, a higher lambda should lead lower error on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lambda_vec, error_train_one, error_train_two, error_train_three, ...\n",
    "          error_val_one, error_val_two, error_val_three] =  validationCurve(XTrain, yTrain, XVal, yVal);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot it:\n",
    "\n",
    "<img src=\"validationCurveClass1.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<img src=\"validationCurveClass2.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<img src=\"validationCurveClass3.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "The plots show that for all learners, a lambda close ro equal to zero would lead to the smallest error! While we will still overfit the training data, we will underfit training and validation set with higher lambdas!\n",
    "\n",
    "## 7. Running the algorithm with (almost) no regularization\n",
    "\n",
    "We will train the learner again, but with lambda = 0.01. Those are the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta =\n",
    "\n",
    "     -4.4261     -3.9377     -7.4192\n",
    "      2.8507     -4.1419      1.5145\n",
    "      1.4751    -0.95095     0.63745\n",
    "       1.771     -4.5732      3.1324\n",
    "     -3.4651      3.3024  -0.0095732\n",
    "      0.2498    -0.22923     0.53593\n",
    "     0.56979    -0.74084      0.8175\n",
    "      1.9058      1.6677     -5.4752\n",
    "    -0.82705      1.2502    -0.67747\n",
    "     -0.7241      1.2882     -1.2522\n",
    "     0.18916     -4.5637      2.0422\n",
    "     0.70584      2.6196     -2.8687\n",
    "      1.9754     0.77589     -2.9374\n",
    "       4.487     -5.4983      0.3082\n",
    "\n",
    "Press Enter to continue.\n",
    "Costs for the training set:\n",
    "cost_one =  0.0036292\n",
    "cost_two =  0.0080645\n",
    "cost_three =  0.0044789"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the parameters theta are markedly higher than with lambda = 1. Moreover, the training costs have sunk by one to two orders of magnitude, so we did indeed underfit with our first run.\n",
    "\n",
    "This is also borne out by the learning curves:\n",
    "\n",
    "<img src=\"LearningCurveLambda0.01Class1.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<img src=\"LearningCurveLambda0.01Class2.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<img src=\"LearningCurveLambda0.01Class3.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "## 8. Predicting classes of new examples\n",
    "\n",
    "We've trained the learners, now it's time to make predictions!\n",
    "\n",
    "This function takes the validation set and lets, for each example, each learner assign probabilities of the example belonging to its class. The class with the highest probability is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict makes a prediction for each example\n",
    "[pVal, hypo] = predict(XVal, theta_one, theta_two, theta_three, yVal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the predictions with the actual classes. There are many ways to measure classification performance, such as true and false positives, precision, recall, F1, Area-under-curve etc. In his book \"Machine Learning Yearning\", Andrew Ng urges the use of a single-value metric. We heed his advice (for now) and start by measuring the overall accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctVal = sum(pVal == yVal);\n",
    "accuracyVal = correctVal / length(yVal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get as result:\n",
    "\n",
    "accuracyVal =    0.97143\n",
    "\n",
    "Since we have 35 examples in the validation set, that's 34 out of 35. Pretty good!\n",
    "\n",
    "For the test set we get the same results:\n",
    "\n",
    "accuracyTest =    0.97143\n",
    "\n",
    "With those results, I don't think there's a need to delve deeper in to performance metrics. In the same vein, there's not much of a point to try other types of algorithms: This one does the job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
